{
    "protein_graph_conv_layer": "dgl_tag",
    "ligand_graph_conv_layer": "dgl_tag",
    "protein_input_size": 74,
    "ligand_input_size": 74,
    "protein_graph_conv_dims": [50, 45],
    "ligand_graph_conv_dims": [50, 45, 45],
    "sequence_length": 150,
    "embedding_dim": 45,
    "ligand_graph_pooling": "dgl_maxpool",
    "protein_graph_pooling": "dgl_maxpool",
    "use_lstm_layer": false,
    "use_bilstm": false,
    "lstm_input": null,
    "lstm_output": null,
    "lstm_num_layers": null,
    "lstm_dropout_rate": null,
    "head_dims": [2000, 1000, 500, 1],
    "attention_layer": "transformer_attention",
    "attention_head": 1,
    "attention_dropout": 0.0,
    "qk_scale": null,
    "proj_drop": 0.0,
    "attention_layer_bias": true,
    "protein_conv_dropout_rate": [0.2, 0.2],
    "protein_conv_normalization": ["instance_norm1d", "instance_norm1d"],
    "ligand_conv_normalization": ["instance_norm1d", "instance_norm1d", "instance_norm1d"],
    "protein_graph_conv_kwargs": [{"k": 8}, {"k": 8}],
    "ligand_graph_conv_kwargs": [{"k": 8}, {"k": 8}, {"k": 8}],
    "ligand_graph_pooling_kwargs": {},
    "protein_graph_pooling_kwargs": {},
    "ligand_conv_dropout_rate": [0.2, 0.2, 0.2],
    "head_dropout_rate": 0.1,
    "head_activation_fn": "relu"
}
